{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition: Lab 5\n",
    "### Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.89950217, -0.90955811, -0.35495698],\n",
      "       [ 0.04266477,  0.00642001, -0.33235504],\n",
      "       [-0.40823321, -0.14182853,  0.26392577]]), array([[-0.98484024],\n",
      "       [-0.97173261],\n",
      "       [ 0.44208069]])]\n",
      "('epochs:', 0)\n",
      "('epochs:', 10000)\n",
      "('epochs:', 20000)\n",
      "('epochs:', 30000)\n",
      "('epochs:', 40000)\n",
      "('epochs:', 50000)\n",
      "('epochs:', 60000)\n",
      "('epochs:', 70000)\n",
      "('epochs:', 80000)\n",
      "('epochs:', 90000)\n",
      "(array([0, 0]), 0)\n",
      "(array([0, 1]), 1)\n",
      "(array([1, 0]), 1)\n",
      "(array([1, 1]), 0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6RGGfCS1CgDXpIaZcBLUqMMeElq1P8DeJ26dVWjSqIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c3aacd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.errors = []\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        \n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) - 1\n",
    "            self.weights.append(r)\n",
    "            # output layer - random((2+1, 1)) : 3 x 1\n",
    "            r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "            self.weights.append(r)\n",
    "            \n",
    "        print(self.weights)\n",
    "            \n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000, momentum=1):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            allError = 0;\n",
    "            for i in range(X.shape[0]):\n",
    "                a = [X[i]]\n",
    "\n",
    "                for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "                # output layer\n",
    "                error = y[i]\n",
    "                if a[-1] > 0.5:\n",
    "                    error -= 1\n",
    "                allError += abs(error)\n",
    "                deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "                # we need to begin at the second to last layer \n",
    "                # (a layer before the output layer)\n",
    "                for l in range(len(a) - 2, 0, -1): \n",
    "                    deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "                # reverse\n",
    "                # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "                deltas.reverse()\n",
    "\n",
    "                # backpropagation\n",
    "                # 1. Multiply its output delta and input activation \n",
    "                #    to get the gradient of the weight.\n",
    "                # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "                prev_weights_delta = [0] * len(self.weights)\n",
    "                for i in range(len(self.weights)):\n",
    "                    layer = np.atleast_2d(a[i])\n",
    "                    delta = np.atleast_2d(deltas[i])\n",
    "#                     self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "                    \n",
    "                    # ref: https://jamesmccaffrey.wordpress.com/2017/06/06/neural-network-momentum/\n",
    "                    delta = learning_rate * layer.T.dot(delta)\n",
    "                    self.weights[i] += delta\n",
    "                    self.weights[i] += momentum * prev_weights_delta[i]\n",
    "                    prev_weights_delta[i] = delta\n",
    "                                        \n",
    "            self.errors.append(allError/4)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "                \n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))\n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        \n",
    "        res = 0\n",
    "        if a > 0.5:\n",
    "            res = 1\n",
    "        return res\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "        \n",
    "    plt.plot(range(len(nn.errors)),nn.errors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
